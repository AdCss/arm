---
title: "Chapter 3, Problem 1 (Gelman & Hill)"
author: "Gianluca Rossi"
date: "26 October 2015"
output: html_document
---

The folder `pyth` contains outcome y and inputs x1, x2 for 40 data points, with a further 20 points with the inputs but no observed outcome. Save the file to your working directory and read it into R using the read.table() function.

a) Use R to fit a linear regression model predicting y from x1,x2, using the first 40 data points in the file. Summarize the inferences and check the fit of your model.
b) Display the estimated model graphically. 
c) Make a residual plot for this model. Do the assumptions appear to be met?
d) Make predictions for the remaining 20 data points in the file. How confident do you feel about these predictions?

### Part a

```{r, message=FALSE, cache=FALSE}
# load required packages
require(foreign)
require(arm)
```

```{r, cache=TRUE}
# download dataset and split it into training and test sets
df <- read.table("http://www.stat.columbia.edu/~gelman/arm/examples/pyth/exercise2.1.dat", header=TRUE)

train <- df[0:40, ]
test <- df[41:60, ]

# fit linear regression model and display results
m1 <- lm(y ~ x1 + x2, data=train)
display(m1)
layout(matrix(c(1,2,3,4),2,2))
plot(m1)
```

The model reports a R-squared of 0.97 which means it describes 97% of the dataset variance. This is an extremely good results. Looking at the residuals we can see they are not normally distributed but they are within comfortable boudaries. In particular observation #8 seems to be an outilier. 

```{r}
new.train <- train[-8, ]
m2 <- lm(y ~ x1 + x2, data=new.train)
display(m2)
layout(matrix(c(1,2,3,4),2,2)) 
plot(m2)
```

Removing observation #8 from the training set we can slightly improve the fit of our initial model. However, the main argument for removing it should be that the observation is more than three standard deviations from the mean. In certain situations outliers like this one should be kept in the training set because they are actually possible observations. In this case we don't know enough about the data to make an informative decision, therefore we won't remove it from the training set.

### Part b

The code in the next chuck will display this model as two plots, one for each of the two input variables with the other held at its average value. This will help us diagnosis if any of the variables should be transformed (log transformation, second power, etc..).

```{r}
beta.hat <- coef(m1) 
beta.sim <- sim(m1)@coef
par(mfrow=c(1,2))

plot (train$x1, train$y, xlab="x1 score", ylab="y score") 
for (i in 1:10){
  curve (cbind (1, mean(train$x2), x) %*% beta.sim[i,], lwd=.5, col="gray", add=TRUE)
}
curve (cbind (1, mean(train$x2), x) %*% beta.hat, col="black", add=TRUE)

plot (train$x2, train$y, xlab="x2 score", ylab="y score") 
for (i in 1:10){
  curve (cbind (1, x, mean(train$x1)) %*% beta.sim[i,], lwd=.5, col="gray", add=TRUE)
}
curve (cbind (1, x, mean(train$x1)) %*% beta.hat, col="black", add=TRUE)
```

### Part c

The residuals aren not perfectly normally distributed. This might affect the predicting power of the model. We saw that removing variable `x1` from the model, will improved the residuals, but at the same time reduce the R-squared on the training set. We should however be aware that the R-squared on the training set has only an indicative power, because it over-estimate the quality of the model. The only way to assess the quality of the model is through cross-validation on unseen data during the training part. 

```{r}
par(mfrow=c(2,2))
plot(m1$residuals, m1$fitted.values) 
plot(m1$residuals, train$x1) 
plot(m1$residuals, train$x2) 
qqnorm(m1$residuals); qqline(m1$residuals) # residuals are not normally distributed
```

### Part d

I wouldn't be so optimistic about the predicting power of the model because residuals are not normally distributed. However, in absence of a better model, `m1` should give us at least an intuition about what sort of outcome we might expect, given the observed indipendent variables.

```{r}
predict (m1, test, interval="prediction", level=0.95)
```
